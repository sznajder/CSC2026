{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSRs9A-sxnlt"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sznajder/CSC2026/blob/main/JetTagging_DS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7_esAc6K1Z4"
   },
   "source": [
    "# Training a Jet Tagging **DeepSets**\n",
    "\n",
    "---\n",
    "In this notebook, we perform a Jet identification task using a DeepSets multiclass classifier.\n",
    "The problem consists in identifying a given jet as a quark, a gluon, a W, a Z, or a top.\n",
    "We will represent each jet as a point cloud (or a \"particle cloud\") which consiststs of unordered set of jet's consistuent particles. Such a particle cloud representation of jets is efficient in incorporating raw information of jets and also explicitly respects the permutation symmetry.\n",
    "\n",
    "Based on https://github.com/jngadiub/ML_course_Pavia_23\n",
    "\n",
    "Modified by: Andre Sznajder\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1730974615679,
     "user": {
      "displayName": "Andre Sznajder",
      "userId": "12562331206892861623"
     },
     "user_tz": 180
    },
    "id": "5pK5FCdMK1Z7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import glob, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVO0EQALK1Z8"
   },
   "source": [
    "# Preparation of the training and validation samples\n",
    "\n",
    "---\n",
    "In order to import the dataset, we now\n",
    "- clone the dataset repository (to import the data in Colab)\n",
    "- load the h5 files in the data/ repository\n",
    "- extract the data we need: a target and jetImage\n",
    "\n",
    "To type shell commands, we start the command line with !\n",
    "\n",
    "**nb, if you are running locally and you have already downloaded the datasets you can skip the cell below and, if needed, change the paths later to point to the folder with your previous download of the datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25790,
     "status": "ok",
     "timestamp": 1730974642503,
     "user": {
      "displayName": "Andre Sznajder",
      "userId": "12562331206892861623"
     },
     "user_tz": 180
    },
    "id": "GlxKaXA8K1Z-",
    "outputId": "8a30814d-0065-4d79-bb33-1f63e0fd0444"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"Data-MLtutorial/JetDataset\"):\n",
    "    print(\"Dataset not found locally. Downloading and extracting...\")\n",
    "    os.system(\"curl -L https://cernbox.cern.ch/s/Hixs6KpgPFH8MN9/download -o Data-MLtutorial.tar.gz\")\n",
    "    os.system(\"tar -zxvf Data-MLtutorial.tar.gz\")\n",
    "    os.remove(\"Data-MLtutorial.tar.gz\")\n",
    "else:\n",
    "    print(\"Dataset already exists. Skipping download.\")\n",
    "\n",
    "os.system(\"ls -al Data-MLtutorial/JetDataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4651,
     "status": "ok",
     "timestamp": 1730974647151,
     "user": {
      "displayName": "Andre Sznajder",
      "userId": "12562331206892861623"
     },
     "user_tz": 180
    },
    "id": "_Bfy4kz2K1Z_",
    "outputId": "27bc4c2c-d51d-4232-9c75-cc9e9acb15ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_30000_40000.h5\n",
      "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_60000_70000.h5\n",
      "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_50000_60000.h5\n",
      "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_10000_20000.h5\n",
      "Appending Data-MLtutorial/JetDataset/jetImage_7_100p_0_10000.h5\n",
      "(50000, 5) (50000, 100, 16)\n"
     ]
    }
   ],
   "source": [
    "target = np.array([])\n",
    "jetList = np.array([])\n",
    "feat_names = dict()\n",
    "# we cannot load all data on Colab. So we just take a few files\n",
    "datafiles = ['Data-MLtutorial/JetDataset/jetImage_7_100p_30000_40000.h5',\n",
    "             'Data-MLtutorial/JetDataset/jetImage_7_100p_60000_70000.h5',\n",
    "             'Data-MLtutorial/JetDataset/jetImage_7_100p_50000_60000.h5',\n",
    "             'Data-MLtutorial/JetDataset/jetImage_7_100p_10000_20000.h5',\n",
    "             'Data-MLtutorial/JetDataset/jetImage_7_100p_0_10000.h5']\n",
    "# if you are running locallt, you can use the full dataset doing\n",
    "# for fileIN in glob.glob(\"tutorials/HiggsSchool/data/*h5\"):\n",
    "first=0\n",
    "for fileIN in datafiles:\n",
    "    print(\"Appending %s\" %fileIN)\n",
    "    f = h5py.File(fileIN)\n",
    "    myJetList = np.array(f.get(\"jetConstituentList\"))\n",
    "    mytarget = np.array(f.get('jets')[0:,-6:-1])\n",
    "    jetList = np.concatenate([jetList, myJetList], axis=0) if jetList.size else myJetList\n",
    "    target = np.concatenate([target, mytarget], axis=0) if target.size else mytarget\n",
    "    #save particles/nodes features names and their indecies in a dictionary\n",
    "    if first==0:\n",
    "      for feat_idx,feat_name in enumerate(list(f['particleFeatureNames'])[:-1]):\n",
    "        feat_names[feat_name.decode(\"utf-8\").replace('j1_','')] = feat_idx\n",
    "      first=1\n",
    "    del myJetList, mytarget\n",
    "    f.close()\n",
    "print(target.shape, jetList.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aohh1ChNYmaD"
   },
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "The ground truth is incorporated in the ['j_g', 'j_q', 'j_w', 'j_z', 'j_t] vector of boolean, taking the form\n",
    "*  [1, 0, 0, 0, 0] for gluons\n",
    "*  [0, 1, 0, 0, 0] for quarks\n",
    "*  [0, 0, 1, 0, 0] for W\n",
    "*  [0, 0, 0, 1, 0] for Z\n",
    "*  [0, 0, 0, 0, 1] for top quarks\n",
    "\n",
    "This is what is called 'one-hot' encoding of a descrete label (typical of ground truth for classification problems). These labels are the 'target' for our classification tasks. Let's convert it back to single-column encoding :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(target))\n",
    "label_names= [\"gluon\", \"quark\", \"W\", \"Z\", \"top\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx5Z9g5JYmaE"
   },
   "source": [
    "## Jet Constituents features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730974647151,
     "user": {
      "displayName": "Andre Sznajder",
      "userId": "12562331206892861623"
     },
     "user_tz": 180
    },
    "id": "QTuZKlYlK1aA",
    "outputId": "3b934395-6546-4e4f-ec45-003cdc883612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particles/Nodes considered features :  ['etarel', 'phirel', 'pt', 'e', 'ptrel', 'erel', 'deltaR']\n"
     ]
    }
   ],
   "source": [
    "feat_to_consider = 'etarel,phirel,pt,e,ptrel,erel,deltaR'.split(',')\n",
    "feat_idx = [feat_names[name] for name in feat_to_consider]\n",
    "jetList = jetList[:,:,feat_idx]\n",
    "print('Particles/Nodes considered features : ',feat_to_consider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbsOXZGNCeLc"
   },
   "source": [
    "## Prepare the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 578,
     "status": "ok",
     "timestamp": 1730974647726,
     "user": {
      "displayName": "Andre Sznajder",
      "userId": "12562331206892861623"
     },
     "user_tz": 180
    },
    "id": "zjq1a6-wYmaD",
    "outputId": "3cf6c30a-feb1-4690-ce27-8f29534d7abb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33500, 100, 7) (16500, 100, 7) (33500, 5) (16500, 5)\n",
      "Jets shape :  (50000, 100, 7)\n",
      "Target/Labels shape :  (50000, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(jetList, target, test_size=0.33)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "print('Jets shape : ',jetList.shape)\n",
    "print('Target/Labels shape : ',target.shape)\n",
    "del jetList, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1730974647727,
     "user": {
      "displayName": "Andre Sznajder",
      "userId": "12562331206892861623"
     },
     "user_tz": 180
    },
    "id": "RaUzTTuYK1aG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.17.0\n"
     ]
    }
   ],
   "source": [
    "# keras imports\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, BatchNormalization, Activation\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "print(f\"TensorFlow {tf.__version__}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Attention implementation from Stefania Cristina:\n",
    "# https://machinelearningmastery.mystagingwebsite.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/\n",
    "#\n",
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
    "from keras.layers import Dense, Layer\n",
    "from keras.backend import softmax\n",
    "\n",
    "\n",
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    "\n",
    "# Implementing the Multi-Head Attention\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.heads = h  # Number of attention heads to use\n",
    "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model  # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
    "\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    "\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "\n",
    "        # Apply one final linear projection to the output to generate the multi-head attention\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Attention implementation from Sebastian W.\n",
    "#\n",
    "from tensorflow import keras\n",
    "\n",
    "class Attn(keras.layers.Layer):\n",
    "    def __init__(self, dim = 16, nhead = 1, nbits = 8, **kwargs):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "#        self.dim_out = dim_out\n",
    "        self.nhead = nhead\n",
    "        self.nbits = nbits\n",
    "\n",
    "        # Set QKeras quantizer \n",
    "        if nbits == 1:\n",
    "            qbits = 'binary(alpha=1)'\n",
    "        elif nbits == 2:\n",
    "            qbits = 'ternary(alpha=1)'\n",
    "        else:\n",
    "            qbits = 'quantized_bits({},0,alpha=1)'.format(nbits)\n",
    "            \n",
    "        \n",
    "        self.qD   = QDense(dim,  kernel_quantizer=qbits, bias_quantizer=qbits, name='attn_query')\n",
    "        self.kD   = QDense(dim,  kernel_quantizer=qbits, bias_quantizer=qbits, name='attn_key')\n",
    "        self.vD   = QDense(dim,  kernel_quantizer=qbits, bias_quantizer=qbits, name='attn_value')\n",
    "        self.outD = QDense(dim,  kernel_quantizer=qbits, bias_quantizer=qbits, name='attn_out')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"dim\": self.dim,\n",
    "            \"nhead\": self.nhead,\n",
    "            \"nbits\": self.nbits,\n",
    "        })\n",
    "        return config\n",
    "        \n",
    "    \n",
    "    def call(self, input):\n",
    "        input_shape = input.shape\n",
    "        shape_ = (-1, input_shape[1], self.nhead, self.dim//self.nhead)\n",
    "        perm_ = (0, 2, 1, 3)\n",
    "\n",
    "        print (\"Attention Input shape = \", input_shape)\n",
    "        \n",
    "        q = self.qD(input)\n",
    "        q = tf.reshape(q, shape = shape_)\n",
    "        q = tf.transpose(q, perm = perm_)\n",
    "\n",
    "        print (\"Attention Reshaped shape = \", q.shape)\n",
    "\n",
    "        \n",
    "#        k = self.qD(input) # Bug from Sebastian code\n",
    "        k = self.kD(input)\n",
    "        k = tf.reshape(k, shape = shape_)\n",
    "        k = tf.transpose(k, perm = perm_)\n",
    "\n",
    "#        v = self.qD(input) # Bug from Sebastian code\n",
    "        v = self.vD(input)\n",
    "        v = tf.reshape(v, shape = shape_)\n",
    "        v = tf.transpose(v, perm = perm_)\n",
    "\n",
    "        a = tf.matmul(q, k, transpose_b=True)\n",
    "        a = tf.nn.softmax(a / q.shape[3]**0.5, axis = 3)\n",
    "\n",
    "        out = tf.matmul(a, v)\n",
    "        out = tf.transpose(out, perm = perm_)\n",
    "        out = tf.reshape(out, shape = (-1, input_shape[1], self.dim))\n",
    "        out = self.outD(out)\n",
    "\n",
    "        print(\"Attention output shape = \",out.shape)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My simplified Attention implementation\n",
    "import tensorflow as tf\n",
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
    "from keras.layers import Dense, Layer\n",
    "from keras.ops import softmax\n",
    "\n",
    "class Attn(keras.layers.Layer):\n",
    "    def __init__(self, dim = 16, nhead = 1, **kwargs):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "#        self.dim_out = dim_out\n",
    "        self.nhead = nhead\n",
    "\n",
    "            \n",
    "        self.vD   = Dense(dim, name='attn_value')\n",
    "        self.outD = Dense(dim, name='attn_out')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({ \"dim\": self.dim, \"nhead\": self.nhead })\n",
    "        return config\n",
    "\n",
    "    \n",
    "    def call(self, input):\n",
    "        input_shape  = input.shape\n",
    "        print(\"input shape = \",input_shape)\n",
    "\n",
    "        output_shape = (input_shape[0], input_shape[1], self.dim)\n",
    "        print(\"output shape = \",output_shape)\n",
    "        \n",
    "#        shape_ = (-1, input_shape[1], self.nhead, self.dim//self.nhead)\n",
    "        shape_ = (input_shape[0], input_shape[1], self.nhead, -1)\n",
    "        perm_ = (0, 2, 1, 3)\n",
    "\n",
    "        v = self.vD(input)\n",
    "        v = tf.reshape(v, shape = shape_)\n",
    "        v = tf.transpose(v, perm = perm_)\n",
    "\n",
    "        a = tf.matmul(v, v, transpose_b=True)\n",
    "        a = tf.nn.softmax(a / v.shape[3]**0.5, axis = 3)\n",
    "\n",
    "        out = tf.matmul(a, v)\n",
    "        out = tf.transpose(out, perm = perm_)\n",
    "        out = tf.reshape(out, shape = output_shape)\n",
    "        out = self.outD(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1730974647727,
     "user": {
      "displayName": "Andre Sznajder",
      "userId": "12562331206892861623"
     },
     "user_tz": 180
    },
    "id": "xjQH0sMuK1aI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape =  (None, 100, 7)\n",
      "output shape =  (None, 100, 24)\n",
      "input shape =  (None, 100, 7)\n",
      "output shape =  (None, 100, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sznajder/WorkM1/miniforge3/envs/tf2_17/lib/python3.11/site-packages/keras/src/layers/layer.py:1381: UserWarning: Layer 'attn_5' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Failed to convert elements of (None, 100, 24) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.''\n",
      "  warnings.warn(\n",
      "/Users/sznajder/WorkM1/miniforge3/envs/tf2_17/lib/python3.11/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'attn_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling Attn.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'attn_5' (of type Attn). Either the `Attn.call()` method is incorrect, or you need to implement the `Attn.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nFailed to convert elements of (None, 100, 24) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\u001b[0m\n\nArguments received by Attn.call():\n  • args=('<KerasTensor shape=(None, 100, 7), dtype=float32, sparse=False, name=keras_tensor_5>',)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m dim \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m\n\u001b[1;32m     19\u001b[0m nhead\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[0;32m---> 20\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[43mAttn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Phi MLP ( permutation equivariant layers )\u001b[39;00m\n\u001b[1;32m     23\u001b[0m h \u001b[38;5;241m=\u001b[39m Dense(nnodes_phi)(h)\n",
      "File \u001b[0;32m~/WorkM1/miniforge3/envs/tf2_17/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[18], line 45\u001b[0m, in \u001b[0;36mAttn.call\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     43\u001b[0m out \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(a, v)\n\u001b[1;32m     44\u001b[0m out \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(out, perm \u001b[38;5;241m=\u001b[39m perm_)\n\u001b[0;32m---> 45\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutD(out)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling Attn.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'attn_5' (of type Attn). Either the `Attn.call()` method is incorrect, or you need to implement the `Attn.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nFailed to convert elements of (None, 100, 24) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\u001b[0m\n\nArguments received by Attn.call():\n  • args=('<KerasTensor shape=(None, 100, 7), dtype=float32, sparse=False, name=keras_tensor_5>',)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "nnodes_phi = 32\n",
    "nnodes_rho = 16\n",
    "activ      = \"relu\"\n",
    "\n",
    "# Input tensor shape\n",
    "nconstit = X_train.shape[1]\n",
    "nfeat = X_train.shape[2]\n",
    "\n",
    "\n",
    "# Instantiate Tensorflow input tensors in Batch mode\n",
    "inp = Input(shape=(nconstit,nfeat), name=\"inp\")   # Conv1D input format\n",
    "\n",
    "\n",
    "# Input point features BatchNormalization\n",
    "h = BatchNormalization(name='BatchNorm')(inp)\n",
    "\n",
    "# Attention\n",
    "dim =24\n",
    "nhead=8\n",
    "h = Attn(dim,nhead)(h)\n",
    "\n",
    "# Phi MLP ( permutation equivariant layers )\n",
    "h = Dense(nnodes_phi)(h)\n",
    "h = Activation(activ)(h)\n",
    "h = Dense(nnodes_phi)(h)\n",
    "h = Activation(activ)(h)\n",
    "h = Dense(nnodes_phi)(h)\n",
    "phi_out = Activation(activ)(h)\n",
    "\n",
    "\n",
    "# Agregate features (taking mean) over set elements\n",
    "mean = GlobalAveragePooling1D(name='avgpool_1')(phi_out)      # return mean of features over jets constituents\n",
    "\n",
    "# Rho MLP\n",
    "h = Dense(nnodes_rho)(mean)\n",
    "h = Activation(activ)(h)\n",
    "h = Dense(num_classes)(h)\n",
    "out = Activation('softmax',name='softmax_g')(h)\n",
    "\n",
    "# Build the model\n",
    "model = Model(inputs=inp, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "executionInfo": {
     "elapsed": 1119,
     "status": "ok",
     "timestamp": 1730975014640,
     "user": {
      "displayName": "Andre Sznajder",
      "userId": "12562331206892861623"
     },
     "user_tz": 180
    },
    "id": "hXVPnzcTK1aI",
    "outputId": "2d48751a-ce9b-429f-923c-3b1a79c1a7f7"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[metrics.CategoricalAccuracy()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5leMyRciK1aJ"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54532,
     "status": "ok",
     "timestamp": 1730975075136,
     "user": {
      "displayName": "Andre Sznajder",
      "userId": "12562331206892861623"
     },
     "user_tz": 180
    },
    "id": "aj_vANK5K1aK",
    "outputId": "0645d284-ef1a-40d2-b7fa-b6919b81d87a"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "executionInfo": {
     "elapsed": 1236,
     "status": "ok",
     "timestamp": 1730975079591,
     "user": {
      "displayName": "Andre Sznajder",
      "userId": "12562331206892861623"
     },
     "user_tz": 180
    },
    "id": "z7E3JsM_K1aK",
    "outputId": "4c96d2cb-3f2f-43a0-f57a-9fd50f871542"
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2)\n",
    "print(history.history.keys())\n",
    "axes[0].plot(history.history[\"categorical_accuracy\"])\n",
    "axes[0].plot(history.history[\"val_categorical_accuracy\"])\n",
    "axes[0].set_title(\"Accuracy\")\n",
    "axes[0].legend([\"Training\", \"Validation\"])\n",
    "\n",
    "axes[1].plot(history.history[\"loss\"])\n",
    "axes[1].plot(history.history[\"val_loss\"])\n",
    "axes[1].legend([\"Training\", \"Validation\"])\n",
    "axes[1].set_title(\"Loss\")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVNnL-1QK1aM"
   },
   "source": [
    "# Building the ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "executionInfo": {
     "elapsed": 2662,
     "status": "ok",
     "timestamp": 1730975162080,
     "user": {
      "displayName": "Andre Sznajder",
      "userId": "12562331206892861623"
     },
     "user_tz": 180
    },
    "id": "N33vdzeQK1aM",
    "outputId": "f8e11c23-3351-4b82-820f-da1340c40d43"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "predict_val = model.predict(X_val)\n",
    "df = pd.DataFrame()\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "auc1 = {}\n",
    "\n",
    "plt.figure()\n",
    "for i, label in enumerate(label_names):\n",
    "\n",
    "        df[label] = y_val[:,i]\n",
    "        df[label + '_pred'] = predict_val[:,i]\n",
    "\n",
    "        fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred'])\n",
    "\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        plt.plot(tpr[label],fpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
    "plt.semilogy()\n",
    "plt.xlabel(\"sig. efficiency\")\n",
    "plt.ylabel(\"bkg. mistag rate\")\n",
    "plt.ylim(0.000001,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
